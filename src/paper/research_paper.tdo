\contentsline {todo}{Write the introduction with a more detailed description of the historical context (elements of statistical learning) and a more detailed description of the steps in the paper}{1}
\contentsline {todo}{konkreter}{2}
\contentsline {todo}{incorporate prior information}{3}
\contentsline {todo}{Besonders hilfreich, da gewisse Model Selection Verfahren bei sehr gro\IeC {\ss }em p nicht mehr anwendbar sind bzw. technisch nicht mehr m\IeC {\"o}glich (bei best subset selection) siehe Einleitung Diplomarbeit}{4}
\contentsline {todo}{Man k\IeC {\"o}nnte im Appendix ein Minimalbeispiel wie wir es in Computational hatten zur Veranschaulichung geben.}{4}
\contentsline {todo}{Anstelle von if eventuell lieber "for"}{5}
\contentsline {todo}{Brauchen eine Zitierung f\IeC {\"u}r die Subderivatives und Kuhn-Tucker Bedingungen.}{5}
\contentsline {todo}{An dieser Stelle sollten wir kurz und knapp den LARS Algorithmus erw\IeC {\"a}hnen.}{5}
\contentsline {todo}{Brauchen eine Zitierung dieser Bedingungen}{5}
\contentsline {todo}{N\IeC {\"a}her erkl\IeC {\"a}ren warum Lasso es erf\IeC {\"u}llt}{6}
\contentsline {todo}{K\IeC {\"o}nnten ein Bild einf\IeC {\"u}gen.}{6}
\contentsline {todo}{Die penalties beeinflussen sich auch gegenseitig, m\IeC {\"u}ssen wir noch was zu schreiben.}{7}
\contentsline {todo}{Brauchen eine Interpretation der piecewise constant eigenschaft, averaging of the coefficients, au\IeC {\ss }erdem m\IeC {\"u}ssen wir beschreiben wieso diese Eigenschaft wertvoll ist.}{8}
\contentsline {todo}{Formel erkl\IeC {\"a}ren}{9}
\contentsline {todo}{Describe this further.}{11}
\contentsline {todo}{think about different structures of the beta-vector (which supports using one of the estimators)}{12}
\contentsline {todo}{Insert images from simulation and evaluation. M\IeC {\"u}ssen package booktabs benutzen.}{12}
\contentsline {todo}{look for other application with $p>n$, e.g. Spectral Data or datasets from Elements of Statistical Learning}{13}
\contentsline {todo}{Insert code for simulation with lasso and fused lasso as soon as the question is being answered how to optimize over both lambdas}{19}
\contentsline {todo}{Diese Aussage l\IeC {\"a}sst sich verallgemeinern, sodass $\beta _1$ und $\beta _2$ Sch\IeC {\"a}tzer mit verschiedenen Lambdas sein k\IeC {\"o}nnen, w\IeC {\"u}rde ich auch machen, da es ein sehr spezieller Fall ist, dass es zwei L\IeC {\"o}sungen f\IeC {\"u}r das gleiche Lambda gibt}{19}
\contentsline {todo}{Ich finde die Notation in dem Beweis nicht gelungen, f\IeC {\"u}r Fused Lasso hatten wir die Lambdas bisher unten und nicht oben, einige Typos m\IeC {\"u}ssen korrigiert werden.}{19}
